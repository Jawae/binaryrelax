{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-11 test results\n",
    "### float model: 92.13\n",
    "### binary model: 89.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./util/')\n",
    "\n",
    "from get_model import get_model, get_model2\n",
    "from tools import progress_bar\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "from math import ceil\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# from utils.input_pipeline import get_image_folders\n",
    "from train_net import train, optimization_step_float, train_eta\n",
    "from quant import optimization_step_eta, optimization_step\n",
    "\n",
    "torch.cuda.is_available()\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../pytorch-tutorial/data', train=True, \n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../pytorch-tutorial/data', train=False, \n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "LEARNING_RATE = 1e-1  # learning rate for all possible weights\n",
    "WEIGHT_DECAY = 1e-4  # hyperparameter for quantization\n",
    "\n",
    "n_epochs = 200 # total number of epochs\n",
    "m_epochs = 150 # the epoch phase II (turn off relaxation) starts\n",
    "\n",
    "train_size = len(trainloader.dataset.train_labels)\n",
    "batch_size = 128\n",
    "n_batches = int(ceil(train_size/batch_size))\n",
    "n_validation_batches = 100\n",
    "# total number of batches in the train set\n",
    "print \"There are \", n_batches, \" batches in the train set.\"\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        in_channels = 3\n",
    "        x = cfg[0]\n",
    "        layers = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(in_channels, x, kernel_size=3, padding=1)),\n",
    "            ('norm0', nn.BatchNorm2d(x)),\n",
    "            ('relu0', nn.ReLU(inplace=True))\n",
    "        ]))\n",
    "        in_channels = x\n",
    "        \n",
    "        index_pool = 0; index_block = 1\n",
    "        for x in cfg[1:]:\n",
    "            if x == 'M':\n",
    "                layers.add_module('pool%d' % index_pool, \n",
    "                                  nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "                index_pool += 1\n",
    "            else:\n",
    "                layers.add_module('conv%d' % index_block, \n",
    "                                  nn.Conv2d(in_channels, x, kernel_size=3, padding=1)),\n",
    "                layers.add_module('norm%d' % index_block, \n",
    "                                  nn.BatchNorm2d(x)),\n",
    "                layers.add_module('relu%d' % index_block, \n",
    "                                  nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "                index_block += 1\n",
    "#         layers.add_module('avg_pool%d' % index_pool, \n",
    "#                           nn.AvgPool2d(kernel_size=1, stride=1))\n",
    "        return layers\n",
    "    \n",
    "    \n",
    "net = VGG('VGG11') # VGG-11 \n",
    "thisname = 'vgg11_'\n",
    "\n",
    "def load_model(net, name=thisname+'float.t7'):\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/' + name)\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['start_epoch']\n",
    "    return best_acc, start_epoch\n",
    "\n",
    "def load_model_quant(net, name):\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/' + name)\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['start_epoch']\n",
    "    all_G_kernels = checkpoint['G_kernels']\n",
    "    return best_acc, start_epoch, all_G_kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# float model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "net.cuda()\n",
    "cudnn.benchmark = True\n",
    "model, loss, optimizer = get_model2(net, \n",
    "                                   learning_rate=LEARNING_RATE, \n",
    "                                   weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "params_float = [best_acc, start_epoch, thisname+'float.t7']\n",
    "\n",
    "def optimization_step_fn(model, loss, x_batch, y_batch):\n",
    "    return optimization_step_float(\n",
    "        model, loss, x_batch, y_batch, \n",
    "        optimizer = optimizer\n",
    "    )\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80,140],gamma=0.1)\n",
    "\n",
    "all_losses = train(\n",
    "    model, loss, optimization_step_fn,\n",
    "    trainloader, testloader, \n",
    "    params_float,\n",
    "    threshold = 0.001,\n",
    "    n_epochs=200, steps_per_epoch=n_batches, n_validation_batches=n_validation_batches,\n",
    "    lr_scheduler=lr_scheduler\n",
    ")\n",
    "# epoch  train_logloss test_logloss train_accuracy test_accuracy     time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training binary weight model\n",
    "modelname = 'bw.t7' \n",
    "load_float_model = 1 # load float model as the initialization\n",
    "eta = 1 # relaxation parameter\n",
    "eta_rate = 1.04 # growth factor for eta\n",
    "\n",
    "all_G_kernels = []\n",
    "    \n",
    "if load_float_model:\n",
    "    best_acc, start_epoch = load_model(net, name=thisname+'float.t7')\n",
    "    use_cuda = True\n",
    "    net.cuda()\n",
    "    cudnn.benchmark = True\n",
    "    model, loss, optimizer = get_model2(net, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    best_acc = 0\n",
    "    start_epoch = 0\n",
    "else:\n",
    "    best_acc, start_epoch, all_G_kernels = load_model_quant(net, name=thisname+modelname)\n",
    "    use_cuda = True\n",
    "    net.cuda()\n",
    "    cudnn.benchmark = True\n",
    "    model, loss, optimizer = get_model2(net, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "params_quant = [best_acc, start_epoch, thisname+modelname]\n",
    "    \n",
    "if load_float_model:\n",
    "    all_G_kernels = [\n",
    "        Variable(kernel.data.clone(), requires_grad=True)\n",
    "        for kernel in optimizer.param_groups[1]['params']\n",
    "    ]\n",
    "    \n",
    "all_W_kernels = [kernel for kernel in optimizer.param_groups[1]['params']]\n",
    "    \n",
    "kernels = [\n",
    "    {'params': all_G_kernels}\n",
    "]\n",
    "    \n",
    "optimizer_quant = optim.SGD(kernels, lr=0)\n",
    "    \n",
    "def optimization_step_fn(model, loss, x_batch, y_batch):\n",
    "    return optimization_step(\n",
    "        model, loss, x_batch, y_batch,\n",
    "        optimizer_list = [optimizer, optimizer_quant]\n",
    "    )\n",
    "    \n",
    "def optimization_step_fn_eta(model, loss, x_batch, y_batch, eta):\n",
    "    return optimization_step_eta(\n",
    "        model, loss, x_batch, y_batch,\n",
    "        optimizer_list = [optimizer, optimizer_quant],\n",
    "        eta = eta\n",
    "    )\n",
    "    \n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80,140],gamma=0.1)\n",
    "\n",
    "all_losses = train_eta(\n",
    "    model, loss, optimization_step_fn,\n",
    "    [all_W_kernels, all_G_kernels],\n",
    "    trainloader, testloader,\n",
    "    params_quant, optimization_step_fn_eta,\n",
    "    n_epochs=n_epochs, steps_per_epoch=n_batches, n_validation_batches=n_validation_batches,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    eta=eta, eta_rate=eta_rate, m_epochs=m_epochs\n",
    ")\n",
    "# epoch  train_logloss test_logloss train_accuracy test_accuracy     time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
